好的，如果只需要一个新闻抓取器，可以将`DefaultNewsScraper`重命名为`NewsScraper`。以下是更新后的代码：

### 1. `NewsSite` 枚举类

```java
public enum NewsSite {
    SITE_A(1, "https://site-a.com/news", "<h1>(.*?)</h1>", "<div class=\"content\">(.*?)</div>"),
    SITE_B(2, "https://site-b.com/news", "<h2>(.*?)</h2>", "<div class=\"text\">(.*?)</div>"),
    SITE_C(3, "https://site-c.com/news", "<h3>(.*?)</h3>", "<div class=\"article\">(.*?)</div>");

    private final int priority;
    private final String url;
    private final String titleRegex;
    private final String contentRegex;

    NewsSite(int priority, String url, String titleRegex, String contentRegex) {
        this.priority = priority;
        this.url = url;
        this.titleRegex = titleRegex;
        this.contentRegex = contentRegex;
    }

    public int getPriority() {
        return priority;
    }

    public String getUrl() {
        return url;
    }

    public String getTitleRegex() {
        return titleRegex;
    }

    public String getContentRegex() {
        return contentRegex;
    }
}
```

### 2. `NewsScraper` 类

```java
import org.openqa.selenium.By;
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.chrome.ChromeDriver;
import org.openqa.selenium.chrome.ChromeOptions;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

public class NewsScraper {
    private static final Logger logger = LoggerFactory.getLogger(NewsScraper.class);
    private static final DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");

    public void scrapeNews(NewsSite site) {
        System.setProperty("webdriver.chrome.driver", "path/to/chromedriver"); // 替换为你的驱动路径
        ChromeOptions options = new ChromeOptions();
        options.addArguments("--headless"); // 无头模式
        WebDriver driver = new ChromeDriver(options);

        try {
            driver.get(site.getUrl());

            // 等待页面加载
            Thread.sleep(5000); // 可以根据需要调整

            String html = driver.getPageSource();
            Pattern titlePattern = Pattern.compile(site.getTitleRegex());
            Pattern contentPattern = Pattern.compile(site.getContentRegex());
            Matcher titleMatcher = titlePattern.matcher(html);
            Matcher contentMatcher = contentPattern.matcher(html);

            // 当前时间往前推10分钟
            LocalDateTime tenMinutesAgo = LocalDateTime.now().minusMinutes(10);

            while (titleMatcher.find() && contentMatcher.find()) {
                String title = titleMatcher.group(1);
                String content = contentMatcher.group(1);
                String publishTime = extractTime(html); // 从HTML中提取发布时间

                LocalDateTime articleTime = LocalDateTime.parse(publishTime, formatter);

                // 检查发布时间是否在10分钟内
                if (articleTime.isAfter(tenMinutesAgo)) {
                    logger.info("New article found: " + title);
                    logger.info("Content: " + content);
                    NewsStorage.storeNews(title, content);
                } else {
                    logger.info("Article is older than 10 minutes: " + title);
                }
            }
        } catch (Exception e) {
            logger.error("Error scraping news from " + site.getUrl(), e);
        } finally {
            driver.quit(); // 关闭浏览器
        }
    }

    private String extractTime(String html) {
        // 解析HTML以提取发布时间，具体实现根据页面结构
        return "2024-01-01 12:00:00"; // 示例，返回实际的发布时间
    }
}
```

### 3. `HttpClient` 类

```java
public class HttpClient {
    public static String get(String url) {
        // 这里可以返回HTML内容，如果使用Selenium，具体实现已在NewsScraper中
        return ""; // 返回获取的HTML
    }
}
```

### 4. `NewsStorage` 类

```java
import java.util.HashSet;
import java.util.Set;

public class NewsStorage {
    private static Set<String> scrapedTitles = new HashSet<>();

    public static boolean isNewsAlreadyScraped(String title) {
        return scrapedTitles.contains(title);
    }

    public static void storeNews(String title, String content) {
        scrapedTitles.add(title);
        // 可以将数据保存到数据库或文件系统
    }
}
```

### 5. `NewsCrawler` 类

```java
import java.util.Arrays;
import java.util.Comparator;

public class NewsCrawler {
    public void start() {
        Arrays.stream(NewsSite.values())
            .sorted(Comparator.comparingInt(NewsSite::getPriority))
            .parallel()
            .forEach(site -> {
                NewsScraper scraper = new NewsScraper();
                scraper.scrapeNews(site);
                if (NewsStorage.isNewsAlreadyScraped(site.getTitleRegex())) {
                    logger.info("Skipping lower priority sites.");
                    return;
                }
            });
    }

    public static void main(String[] args) throws InterruptedException {
        NewsCrawler crawler = new NewsCrawler();
        while (true) {
            crawler.start();
            Thread.sleep(60 * 1000); // 每分钟抓取一次
        }
    }
}
```

### 6. `UserAgentProvider` 类

```java
import java.util.Random;

public class UserAgentProvider {
    private static final String[] USER_AGENTS = {
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Gecko/20100101 Firefox/89.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"
    };

    public static String getRandomUserAgent() {
        Random random = new Random();
        return USER_AGENTS[random.nextInt(USER_AGENTS.length)];
    }
}
```

### 关键日志说明

- **新文章发现**：当抓取到新文章时，记录日志以便后续分析。
- **文章过期**：如果文章的发布时间超过10分钟，记录相关信息。
- **错误处理**：在抓取过程中捕获异常并记录错误信息，方便调试。

### 依赖

确保在你的项目中添加Selenium和SLF4J的依赖，例如使用Maven：

```xml
<dependency>
    <groupId>org.seleniumhq.selenium</groupId>
    <artifactId>selenium-java</artifactId>
    <version>4.20.0</version> <!-- 根据需要选择最新版本 -->
</dependency>
<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-api</artifactId>
    <version>1.7.32</version>
</dependency>
<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-simple</artifactId>
    <version>1.7.32</version>
</dependency>
```

### 总结

以上代码实现了一个使用Selenium的新闻爬虫，支持无头模式并能够抓取不同网站的新闻信息。希望这些代码对你有帮助！如果有其他问题，欢迎随时询问。